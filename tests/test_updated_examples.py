#!/usr/bin/env python3
"""
Comprehensive test suite for updated examples using the new LLM architecture.
This test ensures all examples work with both new and legacy architectures.
"""

import pytest
import asyncio
import os
import sys
from unittest.mock import Mock, AsyncMock, patch

# Add the project root to Python path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from spoon_ai.chat import ChatBot
from spoon_ai.agents.toolcall import ToolCallAgent
from spoon_ai.agents.spoon_react import SpoonReactAI
from spoon_ai.schema import Message, LLMResponse, ToolCall, Function
from spoon_ai.tools import ToolManager


class TestUpdatedExamples:
    """Test suite for updated examples with new LLM architecture."""
    
    @pytest.fixture
    def mock_llm_response(self):
        """Create a mock LLM response for testing."""
        return LLMResponse(
            content="This is a test response from the LLM.",
            tool_calls=[],
            finish_reason="stop",
            native_finish_reason="stop"
        )
    
    @pytest.fixture
    def mock_chatbot_new(self, mock_llm_response):
        """Create a mock ChatBot with new architecture."""
        chatbot = Mock(spec=ChatBot)
        chatbot.use_llm_manager = True
        chatbot.ask = AsyncMock(return_value="Test response")
        chatbot.ask_tool = AsyncMock(return_value=mock_llm_response)
        return chatbot
    
    @pytest.fixture
    def mock_chatbot_legacy(self, mock_llm_response):
        """Create a mock ChatBot with legacy architecture."""
        chatbot = Mock(spec=ChatBot)
        chatbot.use_llm_manager = False
        chatbot.ask = AsyncMock(return_value="Test response")
        chatbot.ask_tool = AsyncMock(return_value=mock_llm_response)
        return chatbot
    
    def test_chatbot_initialization_new_architecture(self):
        """Test ChatBot initialization with new architecture."""
        with patch('spoon_ai.llm.manager.get_llm_manager') as mock_get_manager:
            mock_manager = Mock()
            mock_get_manager.return_value = mock_manager
            
            chatbot = ChatBot(
                use_llm_manager=True,
                llm_provider="openai",
                model_name="gpt-4.1"
            )
            
            assert chatbot.use_llm_manager is True
            assert hasattr(chatbot, 'llm_manager')
    
    def test_chatbot_initialization_legacy_architecture(self):
        """Test ChatBot initialization with legacy architecture."""
        with patch('spoon_ai.utils.config_manager.ConfigManager'):
            chatbot = ChatBot(
                use_llm_manager=False,
                llm_provider="openai",
                model_name="gpt-4.1"
            )
            
            assert chatbot.use_llm_manager is False
    
    @pytest.mark.asyncio
    async def test_agent_with_new_architecture(self, mock_chatbot_new):
        """Test agent functionality with new LLM architecture."""
        # Create a simple test agent
        agent = ToolCallAgent(
            name="test_agent",
            llm=mock_chatbot_new,
            avaliable_tools=ToolManager([])
        )
        
        # Test agent run
        result = await agent.run("Test request")
        
        # Verify agent used the ChatBot
        mock_chatbot_new.ask_tool.assert_called()
        assert isinstance(result, str)
    
    @pytest.mark.asyncio
    async def test_agent_with_legacy_architecture(self, mock_chatbot_legacy):
        """Test agent functionality with legacy architecture."""
        # Create a simple test agent
        agent = ToolCallAgent(
            name="test_agent",
            llm=mock_chatbot_legacy,
            avaliable_tools=ToolManager([])
        )
        
        # Test agent run
        result = await agent.run("Test request")
        
        # Verify agent used the ChatBot
        mock_chatbot_legacy.ask_tool.assert_called()
        assert isinstance(result, str)
    
    @pytest.mark.asyncio
    async def test_spoon_react_ai_initialization(self):
        """Test SpoonReactAI initialization with architecture fallback."""
        with patch('spoon_ai.agents.spoon_react.create_configured_chatbot') as mock_create:
            # Mock successful new architecture initialization
            mock_chatbot = Mock(spec=ChatBot)
            mock_chatbot.use_llm_manager = True
            mock_create.return_value = mock_chatbot
            
            agent = SpoonReactAI(name="test_agent")
            
            assert agent.llm.use_llm_manager is True
    
    @pytest.mark.asyncio
    async def test_spoon_react_ai_fallback(self):
        """Test SpoonReactAI fallback to legacy architecture."""
        with patch('spoon_ai.agents.spoon_react.create_configured_chatbot') as mock_create:
            # Mock failure then success
            mock_create.side_effect = [
                Exception("New architecture failed"),
                Mock(spec=ChatBot, use_llm_manager=False)
            ]
            
            agent = SpoonReactAI(name="test_agent")
            
            # Should have fallen back to legacy
            assert agent.llm.use_llm_manager is False
    
    def test_backward_compatibility_api(self):
        """Test that the API remains backward compatible."""
        # Test that old initialization patterns still work
        with patch('spoon_ai.utils.config_manager.ConfigManager'):
            # This should work without any changes to existing code
            chatbot = ChatBot()  # Default behavior
            
            # These methods should exist
            assert hasattr(chatbot, 'ask')
            assert hasattr(chatbot, 'ask_tool')
            assert callable(chatbot.ask)
            assert callable(chatbot.ask_tool)
    
    @pytest.mark.asyncio
    async def test_architecture_selection_logic(self):
        """Test the logic for selecting between architectures."""
        with patch('spoon_ai.llm.manager.get_llm_manager') as mock_get_manager:
            mock_manager = Mock()
            mock_get_manager.return_value = mock_manager
            
            # Test explicit new architecture selection
            chatbot_new = ChatBot(use_llm_manager=True)
            assert chatbot_new.use_llm_manager is True
            
            # Test explicit legacy architecture selection
            with patch('spoon_ai.utils.config_manager.ConfigManager'):
                chatbot_legacy = ChatBot(use_llm_manager=False)
                assert chatbot_legacy.use_llm_manager is False
    
    @pytest.mark.asyncio
    async def test_error_handling_consistency(self):
        """Test that error handling is consistent across architectures."""
        # Test with new architecture
        with patch('spoon_ai.llm.manager.get_llm_manager') as mock_get_manager:
            mock_manager = Mock()
            mock_manager.chat = AsyncMock(side_effect=Exception("Test error"))
            mock_get_manager.return_value = mock_manager
            
            chatbot_new = ChatBot(use_llm_manager=True)
            
            with pytest.raises(Exception, match="Test error"):
                await chatbot_new.ask([Message(role="user", content="test")])
        
        # Test with legacy architecture
        with patch('spoon_ai.utils.config_manager.ConfigManager'):
            chatbot_legacy = ChatBot(use_llm_manager=False)
            
            # Should handle errors gracefully (specific error depends on implementation)
            # This test ensures the error handling path exists
            assert hasattr(chatbot_legacy, '_ask_legacy')
    
    def test_configuration_compatibility(self):
        """Test that configuration works with both architectures."""
        # Test that configuration parameters are handled correctly
        with patch('spoon_ai.llm.manager.get_llm_manager') as mock_get_manager:
            mock_manager = Mock()
            mock_get_manager.return_value = mock_manager
            
            # New architecture should accept configuration
            chatbot_new = ChatBot(
                use_llm_manager=True,
                llm_provider="openai",
                model_name="gpt-4.1"
            )
            
            assert chatbot_new.use_llm_manager is True
            assert chatbot_new.model_name == "gpt-4.1"
            assert chatbot_new.llm_provider == "openai"
        
        # Legacy architecture should also accept configuration
        with patch('spoon_ai.utils.config_manager.ConfigManager'):
            chatbot_legacy = ChatBot(
                use_llm_manager=False,
                llm_provider="openai",
                model_name="gpt-4.1"
            )
            
            assert chatbot_legacy.use_llm_manager is False


class TestExampleScripts:
    """Test the actual example scripts to ensure they work."""
    
    @pytest.mark.asyncio
    async def test_my_agent_demo_structure(self):
        """Test that my_agent_demo.py has the correct structure."""
        # Import the example
        try:
            from examples.agent.my_agent_demo import MyInfoAgent, SmartWeatherTool
            
            # Test that classes exist and have correct structure
            assert issubclass(MyInfoAgent, ToolCallAgent)
            assert hasattr(MyInfoAgent, 'name')
            assert hasattr(MyInfoAgent, 'description')
            assert hasattr(MyInfoAgent, 'system_prompt')
            
            # Test tool structure
            tool = SmartWeatherTool()
            assert hasattr(tool, 'name')
            assert hasattr(tool, 'description')
            assert hasattr(tool, 'parameters')
            assert callable(tool.execute)
            
        except ImportError as e:
            pytest.skip(f"Could not import example: {e}")
    
    @pytest.mark.asyncio
    async def test_tavily_search_agent_structure(self):
        """Test that tavily_search_agent.py has the correct structure."""
        try:
            from examples.mcp.tavily_search_agent import TavilySearchAgent
            
            # Test that class exists and has correct structure
            assert issubclass(TavilySearchAgent, SpoonReactAI)
            assert hasattr(TavilySearchAgent, 'name')
            assert hasattr(TavilySearchAgent, 'description')
            assert hasattr(TavilySearchAgent, 'system_prompt')
            
        except ImportError as e:
            pytest.skip(f"Could not import MCP example: {e}")
    
    def test_example_imports(self):
        """Test that all examples can be imported without errors."""
        examples_to_test = [
            'examples.agent.my_agent_demo',
            'examples.mcp.tavily_search_agent',
        ]
        
        for example in examples_to_test:
            try:
                __import__(example)
                print(f"✓ Successfully imported {example}")
            except ImportError as e:
                print(f"⚠ Could not import {example}: {e}")
                # Don't fail the test, just log the issue
    
    @pytest.mark.asyncio
    async def test_architecture_fallback_in_examples(self):
        """Test that examples properly handle architecture fallback."""
        # This test ensures that examples can handle both architectures
        # and fall back gracefully when needed
        
        # Mock the create_configured_chatbot function to test fallback
        with patch('spoon_ai.agents.spoon_react.create_configured_chatbot') as mock_create:
            # First call fails (new architecture), second succeeds (legacy)
            mock_create.side_effect = [
                Exception("New architecture failed"),
                Mock(spec=ChatBot, use_llm_manager=False)
            ]
            
            # This should not raise an exception
            agent = SpoonReactAI(name="test_fallback")
            assert agent.llm.use_llm_manager is False


class TestMigrationCompatibility:
    """Test migration compatibility between architectures."""
    
    @pytest.mark.asyncio
    async def test_message_format_compatibility(self):
        """Test that message formats work across architectures."""
        # Test different message formats
        message_formats = [
            [{"role": "user", "content": "Hello"}],
            [Message(role="user", content="Hello")],
            [{"role": "user", "content": "Hello"}, {"role": "assistant", "content": "Hi"}]
        ]
        
        for messages in message_formats:
            # Test with new architecture (mocked)
            with patch('spoon_ai.llm.manager.get_llm_manager') as mock_get_manager:
                mock_manager = Mock()
                mock_manager.chat = AsyncMock(return_value=Mock(content="Response"))
                mock_get_manager.return_value = mock_manager
                
                chatbot_new = ChatBot(use_llm_manager=True)
                
                # Should not raise an exception
                try:
                    await chatbot_new.ask(messages)
                except Exception as e:
                    # Expected due to mocking, but format should be handled
                    pass
    
    def test_configuration_migration(self):
        """Test that configuration can be migrated between architectures."""
        # Test configuration that should work in both architectures
        config_params = {
            'llm_provider': 'openai',
            'model_name': 'gpt-4.1',
            'enable_prompt_cache': True
        }
        
        # Test new architecture
        with patch('spoon_ai.llm.manager.get_llm_manager') as mock_get_manager:
            mock_manager = Mock()
            mock_get_manager.return_value = mock_manager
            
            chatbot_new = ChatBot(use_llm_manager=True, **config_params)
            assert chatbot_new.use_llm_manager is True
        
        # Test legacy architecture
        with patch('spoon_ai.utils.config_manager.ConfigManager'):
            chatbot_legacy = ChatBot(use_llm_manager=False, **config_params)
            assert chatbot_legacy.use_llm_manager is False


if __name__ == "__main__":
    # Run the tests
    pytest.main([__file__, "-v"])